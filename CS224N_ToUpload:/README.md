# Stanford CS224N Assignments

This repository contains my solutions to **some** of the programming assignments for Stanford's CS224N: Natural Language Processing with Deep Learning (as part of my self-study).

## What's Here
I've completed and uploaded the following two assignments. The code is based on the starter code provided by the course, with the core model implementations filled in by me.

- **Assignment 1: Word2Vec**
  - Implemented the skip-gram model with negative sampling from scratch.
  - Learned about word vector representations and basic PyTorch training loops.

- **Assignment 4: Attention & Transformer Basics**
  - Implemented the key-value attention mechanism.
  - Built and understood the core components of a Transformer model.

**Large File Notice:** 
To comply with GitHub's file size limits, **generated model parameter files (`*.params`) and the `saved_params/` directory are not included** in this repository.

## Note
- This is a **personal learning record**. The solutions might not be perfect or optimal.
- The course materials and starter code belong to the CS224N teaching team.
- I completed these assignments to solidify my understanding of PyTorch and fundamental NLP models.
